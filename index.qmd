---
title: "Developer Working Status - Stack Overflow 2022 Survey"
author: "Nils Indreiten"
format: html
editor: visual
---

## Executive Summary

```{r setup, include=FALSE}
pacman::p_load(tidyverse,tidymodels,gt,readr,tidyquant,vip)
survey <- read_csv("to_import.csv")

values <- c("#EB0050", "#143C75", "#4BB9A0", "#ECBF08")

```

This project is concerned with the Stack Overflow 2022 developer survey, this is a yearly survey conducted by Stack Overflow on developers and their current state of affairs. It covers a range of topics from income to how many years of experience they have and so on. Remote and Hybrid working arrangements have become increasingly important post COVID-19, as hybrid/fully remote arrangements become more popular; it is important that organisations adequately prepare for the. The objectives of this project are as follows. First to engage in exploratory data analysis. Second to understand whether there is a statistically significant difference in the compensation between remote and hybrid developers. Finally to train and screen some models with the view to deploy the best performing one to production.

## Data Understanding

The data set contains more than 50 variables, ranging from years of experience to employment etc. For the purposes of this project we will select only a few and only Germany, The United Kingdom and The United States of America will be considered the variables chosen are outlined below:

```{r}
#| echo: false
survey %>%
  select(
    ResponseId,
    Employment,
    EdLevel,
    YearsCode,
    YearsCodePro,
    OrgSize,
    Country,
    # Gender,
    CompTotal,
    Currency,
    WorkExp,
    RemoteWork
  ) -> survey
colnames(survey)
```

There is some data cleaning that we need to do, for example, consider the difference in the currencies across the countries in our sample:

```{r}
#| echo: false

survey %>% 
  group_by(Country,Currency) %>% 
  tally(name = "Count") %>% 
  knitr::kable()
```

We can see that there is a difference in the currencies between the countries, let's convert the euro and pound sterling to dollar:

```{r}
#| message: false
survey %>%
  mutate(
    Currency = str_sub(Currency, 1, 3),
    Compensation_usd = case_when(
      Currency == "GBP" ~ CompTotal * 1.21,
      Currency == "EUR" ~ CompTotal * 1.06,
      TRUE ~ CompTotal
    )
  ) %>%
  select(-CompTotal,-Currency,-ResponseId) %>%
  mutate(YearsCode = as.double(YearsCode),
         YearsCodePro = as.double(YearsCodePro),
         Country = case_when(
           Country == "United States of America" ~ "USA",
           Country == "United Kingdom of Great Britain and Northern Ireland" ~ "GB",
           TRUE ~ Country
         )) %>% 
  na.omit() %>% 
  filter(Compensation_usd <= 6000000 & Compensation_usd >= 1000) -> survey 
```

The exchange rate was determined at the time of writing this project and may be different at the time of reading this. In addition we also changed the columns denoting the numbers of years with coding experience to be numeric, since they where characters, and shortened the country names. Finally we also removed some outliers.

### Data Viz

Disparities between the compensation among developers depending on geographical region has long been a subject of study. Let's take a look at this difference for the three countries in the sample:

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Figure 1. Distribution of total compensation for developers

# Data Viz one:

survey %>% 
  filter(Compensation_usd>0) %>% 
  ggplot(aes(Compensation_usd, fill=Country)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1000000),
                     labels = scales::number_format(prefix = "$"))+
  scale_y_continuous(expand = c(0.01,0.01))+
  facet_wrap(~Country,scales="free_y")+
  tidyquant::theme_tq()+
  theme(
    axis.text.x = element_text(
      angle=45,
      vjust=1,
      hjust=1
    )
  )+
  scale_fill_manual(values=values)+
  geom_vline(aes(xintercept = 110000))+
  labs(y="Count",
       x="",
       fill= NULL)+
  theme(legend.position = "none",
        plot.title.position = 'plot')
```

At a general level it seems that the distributions in each country follow a similar trend, they are right skewed. When comparing the distributions to the median of the compensation for the sample as a whole it becomes clear that, The United States has a larger share of developers earning more than the median compensation, which is \$ 110,000. In contrast, the majority of developers in both the United Kingdom and Germany have a total compensation that is below the median.

It would be interesting to see how this relationship holds up when considering other factors such as years coding professionally and working status:

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Figure 2. Relationship between work experience and compensastion broken down by work status and years coding as a pro

survey %>%
  filter(Compensation_usd <= 600000) %>% 
  ggplot(aes(
    x = WorkExp,
    y = Compensation_usd,
    color = RemoteWork,
    size = YearsCodePro
  )) +
  geom_point(alpha = 0.5) +
  facet_wrap(~Country, scales = "free_y")+
  scale_y_continuous(labels=scales::number_format())+
  scale_color_manual(values = values)+
theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

It is somewhat difficult to see whether a pattern exists, however, the plot above demonstrates that the sample size for The United States is much larger than the other countries. If we focus on The United Kingdom on may be able to suggest that there are more hybrid developers earning \$200k or more with varying years of professional experience. In contrast, we may say the inverse about developers in Germany. Due to the large sample size for The United States it is much harder to pick out the nuances in the developers earning \$200k or more and what level of professional coding experience and working status they have.

### The relatiopnship between working status and compensation

Focusing on the USA sample, let's look into the effect size of the years of professional coding and work experience, when considering remote working status and when not taking. the effect of that into account.

```{r}
survey_usa <- survey %>% 
  filter(Country == "USA")
set.seed(123)
ignore_remote_usa_intervals <- reg_intervals(Compensation_usd ~ YearsCodePro+
                          WorkExp,
                        data=survey_usa,
                        times = 500)

set.seed(123)
account_for_remote_usa <- 
  reg_intervals(Compensation_usd ~ YearsCodePro+
                 WorkExp+
                 RemoteWork,
               data=survey_usa,
               times = 500)
```

The bootstrapped results show that there is virtually no difference
when accounting for remote status. In other words, there is no difference
between the effect of years professionally coding and years of 
work experience on the compensation in USD. Perhaps more importantly
we see that the number of years professionally coded actually has 
a more positive impact on compensation, whereas work experience has a 
negative effect. Why might this be the case?

```{r}

# Plot bootstrapped intervals:

bind_rows(
  ignore_remote_usa_intervals %>% mutate(Remote = "ignore"), 
  account_for_remote_usa %>% mutate(Remote = "account for remote")
) %>%
  filter(!str_detect(term, "Remote")) %>%
  ggplot(aes(.estimate, term, color = Remote)) +
  geom_vline(xintercept = 0, size = 1.5, lty = 2, color = "gray50") +
  geom_errorbar(size = 1.4, alpha = 0.7,
                aes(xmin = .lower, xmax = .upper)) +
  geom_point(size = 3) +
  scale_x_continuous(labels = scales::dollar) +
  scale_color_manual(values = values) +
  theme(legend.position="bottom") +
  labs(x = "Change in compensation (USD $)", y = NULL, color = "Include working status in model?",
       title = "Bootstrap confidence intervals for developer compensation in the United States",
       subtitle = "There is no difference in the effect of years and work esperience that developers have according to whether they are remote developers or not")+
  tidyquant::theme_tq()
```

## Predictive Analytics

Now that we have a better understanding of the data, let's proceed to 
build some predictive models that will aid us in predicting whether
a developer will opt for being fully remote or hybrid. 

We begin by creating our training and testing splits, using stratified
re-sampling on the outcome variable:


```{r}
set.seed(1234)
Stack_split <- initial_split(survey, strata = RemoteWork)
Stack_train <- training(Stack_split)
Stack_test <- testing(Stack_split)
```

Next we want to specify our pre-processing recipe or our feature 
engineering steps:

```{r}
Stack_recipe <- recipe(RemoteWork ~ ., data = survey) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

We applied two preprocessing steps, the first applies the Yeo-Johnson
transformation to the data and the second normalises the data. One 
could argue that only one is needed, however given the anomalies in
the data, it would be safe to include both. We can also consider this
when selecting the best model as a method for boosting performance.

Let's begin by specifying three models, all of which we will tune to 
get good candidates for the final models. The specs for the models are
outlined below:

```{r}

glm_spec <-
    logistic_reg(penalty = tune(),mixture = 1) |>
    set_engine("glm")

cart_spec <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree_spec <-
  rand_forest(min_n = tune(),trees=1000) |>
  set_engine("ranger") |>
  set_mode("classification")
```

In addition, we will also implement 5-fold cross-validation, to make the model
selection more robust. This is defined below:

```{r}
set.seed(1234)
Stack_folds <- vfold_cv(Stack_train,strata = RemoteWork,v = 5)
```

Next, we specify a workflow set, which includes all of the models that we 
specified. This will apply the recipe across the models.

```{r}
doParallel::registerDoParallel()
workflow_set <-
  workflow_set(
    preproc = list(Stack_recipe),
    models = list(
      glm = glm_spec,
      cart = cart_spec ,
      tree = tree_spec)
  ) 
```

Finally we need to set up a tuning grid such that different values for the 
different parameters can be passed to the relevant models. We set this grid up 
below, assigning the results to `grid_results`. 

```{r}

# Grid:
grid_ctrl <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

grid_results <-
  workflow_set %>%
  workflow_map(
    seed = 1503,
    resamples = Stack_folds,
    grid = 25,
    control = grid_ctrl
  )

```

We now have a set of candidates for our different types of 
models. Let's get a quick summary of the best candidates 
considering the `roc_auc` metric:

```{r}
rank_results(grid_results,
             rank_metric = "roc_auc",
             select_best = TRUE
) |>
  gt()
```

Performance isn't great, our accuracy and area under the curve remain 
between 63% and 68%. It seems a simple logistic regression model will 
do the job, eliminating the need for additional computation that is
required for the other models. By employing a simpler model we will
also be saving costs. Let's extract this model and perform one last
train and test on the data:

```{r}
best_model_id <- "recipe_glm"

best_fit <- 
  grid_results |>
  extract_workflow_set_result(best_model_id) |>
  select_best(metric = "roc_auc")
  
```

We then finalise the workflow, implement the final fit and collect the metrics:

```{r}
final_workflow <-
  workflow_set |>
  extract_workflow(best_model_id) |>
  finalize_workflow(best_fit)

final_fit <-
  final_workflow |>
  last_fit(Stack_split)

final_fit |>
  collect_metrics() |>
  gt()
```

Our accuracy and area under the curve remain at around the same level. Let's
visualise the curve:

```{r}
final_fit %>% 
  collect_predictions() %>%
  roc_curve(RemoteWork,`.pred_Fully remote`) %>% 
  autoplot()

```

```{r}

final_fit %>% 
  pluck(".workflow",1) %>% 
  extract_fit_parsnip() %>% 
  vip(geom = "point", aesthetics = list(fill = values[2], alpha = 0.8)) +
  scale_y_continuous(expand = c(0, 0))+
  theme_bw()
```

The plot above demonstrates the variable importance in for the prediction.
It seems that The country is the most important variable, being almost 30%
more important than the other variables. Compensation in USD and the 
organisation being 2 to 9 employees large also play a significant role in 
predicting whether developers would be fully remote. 

# Towards a Leaner Approach

There are some structural issues with the data, one that particularly
comes to mind is the class imbalance of the outcome variable. Let's
also try to implement a few other tweaks to the workflow to generate better
performance focusing on the logistic regression model. 

First, let's create a workflowset but in this case we will pass two
separate feature engineering recipes to the model, the first will
be the same as the previous one  and the second will also include down 
sampling of the outcome variable.

```{r}
# Redefine splits:
set.seed(1234)
Stack_folds <- vfold_cv(Stack_train,strata = RemoteWork)
```


```{r}
Stack_recipe <- recipe(RemoteWork ~ ., data = survey) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

The spec will remain the same, however we will specify 
a narrower tuning grid, ranging from -3 to 0. First,lets
set up our spec, then the workflow with 2 separate recipes:

```{r}
glmnet_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% set_engine("glmnet")
```



```{r}

wf_set <-
  workflow_set(
    preproc= list(basic = Stack_recipe,
         downsampling = down_sample_spec),
    list(glmnet = glm_spec)
  )

wf_set
```

Let's specify our penalty grid, we make it narrower than the default
which is -10 to 0, instead let's make it -3 to 0. In order to get
a better understanding of the model we will extract a range of 
metrics; accuracy, mn_log_loss, sensitivity, and specificity:

```{r}
narrower_penalty <- penalty(range = c(-3, 0))

doParallel::registerDoParallel()
set.seed(345)
tune_rs <- 
  workflow_map(
    wf_set,
    "tune_grid",
    resamples = Stack_folds,
    grid = 15,
    metrics = metric_set(accuracy, mn_log_loss, sensitivity, specificity),
    param_info = parameters(narrower_penalty)
  )

tune_rs
```

```{r}
autoplot(tune_rs) + 
  theme_light()+
  theme(legend.position = "none")

```

Its not right, let's specify the model again:

```{r}
glmnet_spec <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
```




