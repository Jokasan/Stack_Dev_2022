---
title: "Developer Working Status - Stack Overflow 2022 Survey"
author: "Nils Indreiten"
format: 
  html:
    theme: journal
    code-fold: true
    code-summary: "Show the code"
    toc: true
---

## Executive Summary

```{r setup, include=FALSE}
pacman::p_load(tidyverse,tidymodels,gt,readr,tidyquant,vip,themis)
survey <- read_csv("to_import.csv")

values <- c("#EB0050", "#143C75", "#4BB9A0", "#ECBF08")

```

This project is concerned with the Stack Overflow 2022 developer survey, this is a yearly survey conducted by Stack Overflow on developers and their current state of affairs. It covers a range of topics from income to how many years of experience they have and so on. Remote and Hybrid working arrangements have become increasingly important post COVID-19, as hybrid/fully remote arrangements become more popular; it is important that organisations adequately prepare for the. The objectives of this project are as follows. First to engage in exploratory data analysis. Second to understand whether there is a statistically significant difference in the compensation between remote and hybrid developers. Finally to train and screen some models with the view to deploy the best performing one to production.

## Data Understanding

The data set contains more than 50 variables, ranging from years of experience to employment etc. For the purposes of this project we will select only a few and only Germany, The United Kingdom and The United States of America will be considered the variables chosen are outlined below:

```{r}
#| echo: false
survey %>%
  select(
    ResponseId,
    Employment,
    EdLevel,
    YearsCode,
    YearsCodePro,
    OrgSize,
    Country,
    # Gender,
    CompTotal,
    Currency,
    WorkExp,
    RemoteWork
  ) -> survey
colnames(survey)
```

There is some data cleaning that we need to do, for example, consider the difference in the currencies across the countries in our sample:

```{r}
#| echo: false

survey %>% 
  group_by(Country,Currency) %>% 
  tally(name = "Count") %>% 
  knitr::kable()
```

We can see that there is a difference in the currencies between the countries, let's convert the euro and pound sterling to dollar:

```{r}
#| message: false
#| error: false
survey %>%
  mutate(
    Currency = str_sub(Currency, 1, 3),
    Compensation_usd = case_when(
      Currency == "GBP" ~ CompTotal * 1.21,
      Currency == "EUR" ~ CompTotal * 1.06,
      TRUE ~ CompTotal
    )
  ) %>%
  select(-CompTotal,-Currency,-ResponseId) %>%
  mutate(YearsCode = as.double(YearsCode),
         YearsCodePro = as.double(YearsCodePro),
         Country = case_when(
           Country == "United States of America" ~ "USA",
           Country == "United Kingdom of Great Britain and Northern Ireland" ~ "GB",
           TRUE ~ Country
         )) %>% 
  na.omit() %>% 
  filter(Compensation_usd <= 6000000 & Compensation_usd >= 1000) -> survey 
```

The exchange rate was determined at the time of writing this project and may be different at the time of reading this. In addition we also changed the columns denoting the numbers of years with coding experience to be numeric, since they where characters, and shortened the country names. Finally we also removed some outliers.

### Data Visualisation

Disparities between the compensation among developers depending on geographical region has long been a subject of study. Let's take a look at this difference for the three countries in the sample:

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Figure 1. Distribution of total compensation for developers

# Data Viz one:

survey %>% 
  filter(Compensation_usd>0) %>% 
  ggplot(aes(Compensation_usd, fill=Country)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1000000),
                     labels = scales::number_format(prefix = "$"))+
  scale_y_continuous(expand = c(0.01,0.01))+
  facet_wrap(~Country,scales="free_y")+
  tidyquant::theme_tq()+
  theme(
    axis.text.x = element_text(
      angle=45,
      vjust=1,
      hjust=1
    )
  )+
  scale_fill_manual(values=values)+
  geom_vline(aes(xintercept = 110000))+
  labs(y="Count",
       x="",
       fill= NULL)+
  theme(legend.position = "none",
        plot.title.position = 'plot')
```

At a general level it seems that the distributions in each country follow a similar trend, they are right skewed. When comparing the distributions to the median of the compensation for the sample as a whole it becomes clear that, The United States has a larger share of developers earning more than the median compensation, which is \$ 110,000. In contrast, the majority of developers in both the United Kingdom and Germany have a total compensation that is below the median.

It would be interesting to see how this relationship holds up when considering other factors such as years coding professionally and working status:

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: Figure 2. Relationship between work experience and compensastion broken down by work status

survey %>%
  filter(Compensation_usd <= 600000 & Compensation_usd >0) %>% 
  ggplot(aes(
    x = WorkExp,
    y = Compensation_usd,
    color = RemoteWork,
    # size = YearsCodePro
  )) +
  geom_point(alpha = 0.5) +
  facet_wrap(~Country, scales = "free_y")+
  scale_y_continuous(labels=scales::number_format())+
  scale_color_manual(values = values)+
theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank())+
  labs(y="Comensation ($)")
```

It is somewhat difficult to see whether a pattern exists, however, the plot above demonstrates that the sample size for The United States is much larger than the other countries. If we focus on The United Kingdom on may be able to suggest that there are more hybrid developers earning \$200k or more with varying years of professional experience. In contrast, we may say the inverse about developers in Germany. Due to the large sample size for The United States it is much harder to pick out the nuances in the developers earning \$200k or more and what level of experience and working status they have.

### The relationship between working status and compensation

Focusing on the USA sample, let's look into the effect size of the years of professional coding and work experience, when considering remote working status and when not taking the effect into account:

```{r}
survey_usa <- survey %>% 
  filter(Country == "USA")
set.seed(123)
ignore_remote_usa_intervals <- reg_intervals(Compensation_usd ~ YearsCodePro+
                          WorkExp,
                        data=survey_usa,
                        times = 500)

set.seed(123)
account_for_remote_usa <- 
  reg_intervals(Compensation_usd ~ YearsCodePro+
                 WorkExp+
                 RemoteWork,
               data=survey_usa,
               times = 500)
```

The bootstrapped results show that there is virtually no difference when accounting for remote status. In other words, there is no difference between the effect of years professionally coding and years of work experience on the compensation in USD. Perhaps more importantly we see that the number of years professionally coded actually has a more positive impact on compensation, whereas work experience has a negative effect. Why might this be the case?

```{r}
#| fig-cap: Figure 3. Bootstrapp intervals when accounting for working status

bind_rows(
  ignore_remote_usa_intervals %>% mutate(Remote = "ignore"), 
  account_for_remote_usa %>% mutate(Remote = "account for remote")
) %>%
  filter(!str_detect(term, "Remote")) %>%
  ggplot(aes(.estimate, term, color = Remote)) +
  geom_vline(xintercept = 0, linewidth = 1.5, lty = 2, color = "gray50") +
  geom_errorbar(linewidth = 1.4, alpha = 0.7,
                aes(xmin = .lower, xmax = .upper)) +
  geom_point(size = 3) +
  scale_x_continuous(labels = scales::dollar) +
  scale_color_manual(values = values) +
  theme(legend.position="bottom") +
  labs(x = "Change in compensation (USD $)", y = NULL, color = "Include working status in model?",
       subtitle = "There is no difference in the effect of coding and work experience that developers have\naccording to whether they are remote developers or not")+
  tidyquant::theme_tq()
```

## Predictive Analytics

Now that we have a better understanding of the data, let's proceed to build some predictive models that will aid us in predicting whether a developer will opt for being fully remote or hybrid. We begin by creating our training and testing splits, using stratified re-sampling on the outcome variable:

```{r}
set.seed(1234)
Stack_split <- initial_split(survey, strata = RemoteWork)
Stack_train <- training(Stack_split)
Stack_test <- testing(Stack_split)
```

Next we want to specify our pre-processing recipe or our feature engineering steps:

```{r}
Stack_recipe <- recipe(RemoteWork ~ ., data = survey) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

We applied two preprocessing steps, the first applies the Yeo-Johnson transformation to the data and the second normalises the data. One could argue that only one is needed, however given the anomalies in the data, it would be safe to include both. We can also consider this when selecting the best model as a method for boosting performance.

Let's begin by specifying three models, all of which we will tune to get good candidates for the final models. The specs for the models are outlined below:

```{r}

glm_spec <-
    logistic_reg(mixture = 1) |>
    set_engine("glm")

cart_spec <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree_spec <-
  rand_forest(min_n = tune(),trees=1000) |>
  set_engine("ranger") |>
  set_mode("classification")
```

In addition, we will also implement 5-fold cross-validation, to make the model selection more robust. This is defined below:

```{r}
set.seed(1234)
Stack_folds <- vfold_cv(Stack_train,strata = RemoteWork,v = 5)
```

Next, we specify a workflow set, which includes all of the models that we specified. This will apply the recipe across the models.

```{r}
doParallel::registerDoParallel()
workflow_set <-
  workflow_set(
    preproc = list(Stack_recipe),
    models = list(
      glm = glm_spec,
      cart = cart_spec ,
      tree = tree_spec)
  ) 
```

Finally we need to set up a tuning grid such that different values for the different parameters can be passed to the relevant models. We set this grid up below, assigning the results to `grid_results`.

```{r}

# Grid:
grid_ctrl <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

grid_results <-
  workflow_set %>%
  workflow_map(
    seed = 1503,
    resamples = Stack_folds,
    grid = 25,
    control = grid_ctrl,
  metrics = metric_set(roc_auc,accuracy, sensitivity, specificity)
  )

```

We now have a set of candidates for our different types of models. Let's get a quick summary of the best candidates considering the `roc_auc` metric:

```{r}
rank_results(grid_results,
             rank_metric = "roc_auc",
             select_best = TRUE
) |>
  gt()
```

Performance isn't great, our accuracy and area under the curve remain between 63% and 68%, for the best model candidates. It seems a simple logistic regression model will do the job, eliminating the need for additional computation that is required for the other models. However, it would be wise to consider other aspects of the performance according to other metrics. This is shown in the figure below, broken down by the model type.

```{r}
#| fig-cap: Figure 4. Metric results for workflow sets
grid_results %>% 
  autoplot()+
  theme_minimal()
```

Let's consider, the decision tree that is rankled 25th. In terms of sensitivity the 25th ranked model, which is a decision tree. We can see that is achieves an accuracy score of around 6.40, whilst in terms of sensitivity it achieves around 0.77, however this comes at the expense of specificity which is just below 0.45. Finally, the area under the curve for this model is 0.65. We can see how these metrics fluctuate for the different model candidates, according to their rank.

In addition to looking at how the metrics fluctuate at the overall level, we can also see how the tuned hyper-parameters affect the metrics. Let's consider the tuned parameter of the decision tree,
minimal node size:

```{r}
#| fig-cap: Figure 5. Fluctuation in metrics according to min. node size

grid_results %>%
    extract_workflow_set_result("recipe_tree") %>% 
autoplot()+
  theme_minimal()+
  labs(title = "Minimal Node Size Effect on Metrics")
```

For instance if we consider the area under the curve metric at around node size 38, we can see that it comes with
a dip in specificity. The trade-off in metrics should be considered when focusing on a specific metric.

By employing a simpler model we will also be saving costs. Let's extract this model and perform one last train and test on the data:

```{r}
best_model_id <- "recipe_glm"

best_fit <- 
  grid_results |>
  extract_workflow_set_result(best_model_id) |>
  select_best(metric = "roc_auc")
  
```

We then finalise the workflow, implement the final fit and collect the metrics:

```{r}
final_workflow <-
  workflow_set |>
  extract_workflow(best_model_id) |>
  finalize_workflow(best_fit)

final_fit <-
  final_workflow |>
  last_fit(Stack_split)

final_fit |>
  collect_metrics() |>
  gt()
```

Our accuracy and area under the curve remain at around the same level. Let's visualise the curve:

```{r}
#| fig-cap: Figure 6. Area under the curve for the final model candidate 

final_fit %>% 
  collect_predictions() %>%
  roc_curve(RemoteWork,`.pred_Fully remote`) %>% 
  autoplot()

```

Performance is still not very good but, given everything we have discussed so far it will do. Let's break down the model
performance for each of the classes with a confusion matrix:

```{r}

collect_predictions(final_fit) %>%
  conf_mat(RemoteWork, .pred_class)
```
Let's get the variable importance at the overall level:

```{r}
#| fig-cap: Figure 7. Variable importance for the final model
final_fit %>% 
  pluck(".workflow",1) %>% 
  extract_fit_parsnip() %>% 
  vip(geom = "point", aesthetics = list(fill = values[2], alpha = 0.8)) +
  scale_x_discrete(labels=label_wrap(35)) +
  theme_tq()

```

The plot above demonstrates the variable importance in for the prediction. It seems that The country is the most important variable, being almost 30% more important than the 
other variables. Compensation in USD and the organisation being 2 to 9 employees large also play a significant role in predicting whether developers would be fully remote.

It would be interesting to see to what extent the predictors are influencing the model, this is shown in the figure below:

```{r,fig.height=12}
#| fig-cap: Figure 8. Direction of variable influence on model prediction

extract_fit_engine(final_fit) %>% 
    vi() %>%
    group_by(Sign) %>%
    slice_max(Importance, n = 15) %>%
    ungroup() %>%
    ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) + 
    geom_col() +
    facet_wrap(vars(Sign), scales = "free_y",ncol = 1) +
    scale_y_discrete(labels=label_wrap(40))+
    scale_x_continuous(expand = c(0.001,0.001))+
  scale_fill_manual(values =values)+
    labs(y = NULL) +theme_minimal()+
    theme(legend.position = "none")
```

It seems that whether the developer is located in the USA,the type of the contract 
the developer has, and the organisation being 2-9 employees negatively influence
the prediction of the outcome variable. In contrast,the educational level of the developer, the organisation
being 10,000 employee or more and being located in Germany positively influence
the outcome variable. In other words, the first three variables are drivers of 
developers adopting a hybrid working arrangement, whilst the other three for developers
adopting a fully remote working arrangement. 

# Conclusion, Recommendations and Future Research Directions

This project explores the Stack Over Flow developer survey for 2022. The 
changes in the developer landscape, such as the fact that hybrid working
arrangements are now included in the survey. Although the survey data contains 
a lot of country, it was decided that the project would only focus on USA,
Germany and the UK. After a brief exploration of the data, the effect 
of including working arrangement on compensation was tested, yielding no
statistically significant difference. Finally, predictive analytics was
employed, using a `workflowset` to screen many models at once, ultimately 
selecting the best model. To wrap up the project the variable importance
was considered as well as the direction of the influence on the prediction.

It could be argued that selecting the best random forest model would have 
yielded better performance, since the logistic regression model simply
used re-sampling with no hyperparameter tuning. This is something that can 
be explored in future iterations of the project. 

Furthermore, the data contains some structural issues, such as the compensation
being 0 or absurdly high 1 million+. Whilst this may be possible, it is anticipated
that this is due to nature of the data collection, although care was taken
to clean up the data, it may be useful to further explore how much this may
be an issue. Finally it would also be worth exploring other models, such 
as an `XG Boost` model or playing around with the pre-processing recipe. 

# Session Info

```{r}
sessionInfo() 
```







